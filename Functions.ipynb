{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install cdlib\n",
        "# pip install leidenalg "
      ],
      "metadata": {
        "id": "7TGWIsaCt_Lr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNR8eJzIAiVY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "from matplotlib import rcParams\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "import os.path\n",
        "from os import path\n",
        "import json\n",
        "import sys\n",
        "import networkx as nx\n",
        "import urllib\n",
        "import numpy as nÑ€\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community\n",
        "from community import community_louvain\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML, display_pretty\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "from cdlib import algorithms\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-jiOo1WCMfA"
      },
      "outputs": [],
      "source": [
        "def read_data(path, sep):\n",
        "    \"\"\"\n",
        "    Read DataFrame from filie and converts some columns\n",
        "    Args:\n",
        "      path (str): file path\n",
        "      sep (str): separator\n",
        "    Returns:\n",
        "      DataFrame\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, sep=sep)\n",
        "    df.loc[:, 'AA'] = df['AA'].apply(lambda x: ast.literal_eval(x))\n",
        "    df.loc[:, 'W'] = df['W'].apply(lambda x: ast.literal_eval(x))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def centrality_ind(filenameInput, info, id_col = 'Id'):\n",
        "    \"\"\"\n",
        "    Compute classical centrality indices and merge to info, save in files\n",
        "    Args:\n",
        "      filenameInput (str): filename with edgelist\n",
        "      info (DataFrame): DataFrame with main info\n",
        "      id_col (str): name of column with ID\n",
        "    Returns:\n",
        "      info (DataFrame) with indices\n",
        "   \n",
        "    \"\"\"\n",
        "\n",
        "    Gdirect = nx.read_weighted_edgelist(filenameInput,delimiter='\\t',create_using=nx.DiGraph)\n",
        "\n",
        "    print(\"network loaded\" )\n",
        "\n",
        "    print(\"Pagerank centrality, directed\")\n",
        "\n",
        "    prc = nx.pagerank(Gdirect, alpha=0.9, personalization=None, max_iter=100, tol=1e-04, nstart=None, weight='weight', dangling=None)\n",
        "\n",
        "    orig_stdout = sys.stdout\n",
        "\n",
        "    f = open('out_Pagerank.txt', 'w')\n",
        "\n",
        "    sys.stdout = f\n",
        "\n",
        "    for v in Gdirect.nodes():\n",
        "\n",
        "        print(v,'\\t',prc[v])\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    prc = pd.DataFrame(prc.items())\n",
        "    prc.rename(columns = {0:id_col, 1:'PageRank'}, inplace = True)\n",
        "    prc[id_col] = prc[id_col].astype(float)\n",
        "    info = info.merge(prc, how = 'left', on = id_col)\n",
        "    info.loc[:, 'PageRank rank'] = info['PageRank'].rank(method = 'dense', ascending = False)\n",
        "\n",
        "    print(\"Eigenvector centrality, directed\")\n",
        "\n",
        "    eigc = nx.eigenvector_centrality(Gdirect, max_iter=1000, tol=1e-04, nstart=None, weight='weight')\n",
        "\n",
        "    orig_stdout = sys.stdout\n",
        "\n",
        "    f = open('out_Eigen.txt', 'w')\n",
        "\n",
        "    sys.stdout = f\n",
        "\n",
        "    for v in Gdirect.nodes():\n",
        "\n",
        "        print(v,'\\t',eigc[v])\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    eigc = pd.DataFrame(eigc.items())\n",
        "    eigc.rename(columns = {0:id_col, 1:'Eigen'}, inplace = True)\n",
        "    eigc[id_col] = eigc[id_col].astype(float)\n",
        "    info = info.merge(eigc, how = 'left', on = id_col)\n",
        "    info.loc[:, 'Eigen rank'] = info['Eigen'].rank(method = 'dense', ascending = False)\n",
        "\n",
        "\n",
        "    print(\"Betweenness centrality, directed\")\n",
        "\n",
        "    b=nx.algorithms.centrality.betweenness_centrality(Gdirect)\n",
        "\n",
        "    orig_stdout = sys.stdout\n",
        "\n",
        "    f = open('out_Between.txt', 'w')\n",
        "\n",
        "    sys.stdout = f\n",
        "\n",
        "    for v in Gdirect.nodes():\n",
        "\n",
        "        print(v,'\\t',b[v])\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    b = pd.DataFrame(b.items())\n",
        "    b.rename(columns = {0:id_col, 1:'Between'}, inplace = True)\n",
        "    b[id_col] = b[id_col].astype(float)\n",
        "    info = info.merge(b, how = 'left', on = id_col)\n",
        "    info.loc[:, 'Between rank'] = info['Between'].rank(method = 'dense', ascending = False)\n",
        "\n",
        "    print(\"done \")\n",
        "\n",
        "    return info"
      ],
      "metadata": {
        "id": "kSG6D2J6CTvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def period_ind(id_name, net, info, id_col = 'Id'):\n",
        "   \"\"\"\n",
        "    Compute classical centrality indices by years\n",
        "    Args:\n",
        "      id_name (DataFrame): DataFrame with mapping IDs and Names\n",
        "      net (DataFrame): temporal network with id_col1 (Source), id_col2 (Target), weight and Y (year)\n",
        "      info (DataFrame): DataFrame with main info\n",
        "      id_col (str): name of column with ID\n",
        "    Returns:\n",
        "      pagerank (DataFrame) with PageRank index\n",
        "      eigen (DataFrame) with Eigenvector index\n",
        "      between (DataFrame) with Betweenness index\n",
        "   \n",
        "    \"\"\"\n",
        "    pagerank = id_name.copy()\n",
        "    eigen = id_name.copy()\n",
        "    between = id_name.copy()\n",
        "\n",
        "    for Y in range(2015, 2022, 1):\n",
        "        net_Y = net[net['Y'] == Y]\n",
        "        net_Y[[id_col + '1', id_col + '2', 'weight']].to_csv(f'net_{Y}.csv', sep = '\\t', index = False, header = False)\n",
        "        print(Y)\n",
        "        \n",
        "        info_Y = id_name.copy()\n",
        "\n",
        "        filenameInput = f'net_{Y}.csv'\n",
        "\n",
        "        Gdirect = nx.read_weighted_edgelist(filenameInput,delimiter='\\t',create_using=nx.DiGraph)\n",
        "\n",
        "        print(\"network loaded\" )\n",
        "\n",
        "        print(\"Pagerank centrality, directed\")\n",
        "\n",
        "        prc = nx.pagerank(Gdirect, alpha=0.9, personalization=None, max_iter=100, tol=1e-04, nstart=None, weight='weight', dangling=None)\n",
        "\n",
        "        prc = pd.DataFrame(prc.items())\n",
        "        prc.rename(columns = {0:id_col, 1:Y}, inplace = True)\n",
        "        prc[id_col] = prc[id_col].astype(float)\n",
        "        pagerank = pagerank.merge(prc, how = 'left', on = id_col)\n",
        "\n",
        "        print(\"Eigenvector centrality, directed\")\n",
        "\n",
        "        eigc = nx.eigenvector_centrality(Gdirect, max_iter=1000, tol=1e-04, nstart=None, weight='weight')\n",
        "\n",
        "        eigc = pd.DataFrame(eigc.items())\n",
        "        eigc.rename(columns = {0:id_col, 1:Y}, inplace = True)\n",
        "        eigc[id_col] = eigc[id_col].astype(float)\n",
        "        eigen = eigen.merge(eigc, how = 'left', on = id_col)\n",
        "\n",
        "        print(\"Betweenness centrality, directed\")\n",
        "\n",
        "        b=nx.algorithms.centrality.betweenness_centrality(Gdirect)\n",
        "\n",
        "        b = pd.DataFrame(b.items())\n",
        "        b.rename(columns = {0:id_col, 1:Y}, inplace = True)\n",
        "        b[id_col] = b[id_col].astype(float)\n",
        "        between = between.merge(b, how = 'left', on = id_col)\n",
        "\n",
        "\n",
        "    pagerank.fillna(0, inplace = True)\n",
        "    pagerank = pagerank.merge(info[[id_col, 'PageRank', 'PageRank rank']], on = id_col)    \n",
        "\n",
        "    eigen.fillna(0, inplace = True)\n",
        "    eigen = eigen.merge(info[[id_col, 'Eigen', 'Eigen rank']], on = id_col)\n",
        "\n",
        "    between.fillna(0, inplace = True)\n",
        "    between = between.merge(info[[id_col, 'Between', 'Between rank']], on = id_col)\n",
        "\n",
        "\n",
        "    print(\"done \")\n",
        "        \n",
        "    return pagerank, eigen, between"
      ],
      "metadata": {
        "id": "WRtxvD_FCTxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_statistics(net, isolated, directed = False, dynamic = True):\n",
        "    \"\"\"\n",
        "    Compute graph statistics\n",
        "    Args:\n",
        "      net (DataFrame): temporal network with id_col1 (Source), id_col2 (Target), weight and Y (year)\n",
        "      isolated (DataFrame): isolated vertices by years\n",
        "      directed (bool): is net directed\n",
        "      dynamic (bool): if True then compute metrics by all years else by first year\n",
        "    Returns:\n",
        "      net_metrics (DataFrame)\n",
        "    \"\"\"\n",
        "\n",
        "    net_metrics = pd.DataFrame()\n",
        "    net_prev_year = None\n",
        "    prev_nodes = set()\n",
        "    prev_edges = set()\n",
        "    all_prev_nodes = set()\n",
        "    all_prev_edges = set()\n",
        "\n",
        "    for year in range(2015, 2022):\n",
        "        print(year)\n",
        "        \n",
        "        if not dynamic:\n",
        "            net_year = net\n",
        "            isolated_year = isolated\n",
        "        else:\n",
        "            net_year = net[net['Y'] == year]\n",
        "            isolated_year = isolated[isolated['Y'] == year]\n",
        "\n",
        "        net_year[['AfId1', 'AfId2', 'weight']].to_csv(f'net_{year}.txt', sep = '\\t', index = False, header = False)\n",
        "\n",
        "        if directed:\n",
        "            G_year = nx.read_weighted_edgelist(f'net_{year}.txt', delimiter='\\t', create_using=nx.DiGraph)\n",
        "        else:\n",
        "            G_year = nx.read_weighted_edgelist(f'net_{year}.txt', delimiter='\\t')\n",
        "        \n",
        "        G_year.add_nodes_from(isolated_year['AfId'])\n",
        "        \n",
        "\n",
        "        # Connected components and sizes\n",
        "        if directed:\n",
        "            components = nx.weakly_connected_components(G_year)\n",
        "        else:\n",
        "            components = nx.connected_components(G_year)\n",
        "        comp_df = pd.DataFrame()\n",
        "        i = 1\n",
        "        for comp in components:\n",
        "            for au in comp:\n",
        "                comp_df.loc[au, 'component'] = i\n",
        "            i += 1\n",
        "                    \n",
        "        comp_df = comp_df.reset_index().rename(columns = {'index': 'AfId'})\n",
        "        cnt_comp = comp_df.groupby('component').count().reset_index().rename(columns = {'AfId':'count'})\n",
        "       \n",
        "        # Average path\n",
        "        if directed:\n",
        "            strong_components = nx.strongly_connected_components(G_year)\n",
        "        else:\n",
        "            strong_components = nx.connected_components(G_year)\n",
        "            \n",
        "        \n",
        "\n",
        "        degrees = [val for (node, val) in G_year.degree()]\n",
        "\n",
        "        net_metrics.loc['Number of vertices', year] = len(set(net_year['AfId1']) | set(net_year['AfId2']) | set(isolated_year['AfId']))\n",
        "        net_metrics.loc['Number of edges', year] = len(net_year)\n",
        "        net_metrics.loc['Number of isolated vertices', year] = len(set(isolated_year['AfId']))\n",
        "        net_metrics.loc['Proportion of isolated vertices', year] = len(set(isolated_year['AfId'])) / len(set(net_year['AfId1']) | set(net_year['AfId2']))\n",
        "        net_metrics.loc['Number of components', year] = i - 1\n",
        "        net_metrics.loc['Average degree', year] = np.mean(degrees)\n",
        "        net_metrics.loc['Minimum degree', year] = np.min(degrees)\n",
        "        net_metrics.loc['Maximum degree', year] = np.max(degrees)\n",
        "        net_metrics.loc['Density', year] = nx.density(G_year)\n",
        "        \n",
        "        net_metrics.loc['Size of the biggest component', year] = max(cnt_comp['count'])\n",
        "        net_metrics.loc['Proportion of vertices in the biggest component', year] = max(cnt_comp['count']) / len(set(net_year['AfId1']) | set(net_year['AfId2']) | set(isolated_year['AfId']))\n",
        "        net_metrics.loc['Size of the 2nd biggest component', year] = max(cnt_comp[cnt_comp['count'] != max(cnt_comp['count'])]['count'])\n",
        "        net_metrics.loc['Size of the 3rd biggest component', year] = max(cnt_comp[(cnt_comp['count'] != max(cnt_comp['count'])) & (cnt_comp['count'] != net_metrics.loc['Size of the 2nd biggest component', year])]['count'])\n",
        "        \n",
        "        nodes = set(net_year['AfId1']) | set(net_year['AfId2']) | set(isolated_year['AfId'])\n",
        "        edges = set(net_year[['AfId1', 'AfId2']].to_records()[['AfId1', 'AfId2']].tolist())\n",
        "        \n",
        "        if net_prev_year is not None:\n",
        "            net_metrics.loc['New nodes', year] = len(nodes - all_prev_nodes) / len(nodes)\n",
        "            net_metrics.loc['New edges', year] = len(edges - all_prev_edges) / len(edges)\n",
        "            net_metrics.loc['Remaining nodes', year] = len(nodes & prev_nodes) / len(nodes)\n",
        "            net_metrics.loc['Remaining edges', year] = len(edges & prev_edges) / len(edges)\n",
        "            net_metrics.loc['Reoccurring nodes', year] = len(nodes & (all_prev_nodes - prev_nodes)) / len(nodes)\n",
        "            net_metrics.loc['Reoccurring edges', year] = len(edges & (all_prev_edges - prev_edges)) / len(edges)\n",
        "            \n",
        "\n",
        "        net_prev_year = net_year.copy()\n",
        "        prev_nodes = nodes.copy()\n",
        "        prev_edges = edges.copy()\n",
        "        all_prev_nodes = all_prev_nodes | nodes\n",
        "        all_prev_edges = all_prev_edges | edges\n",
        "        \n",
        "        if not dynamic:\n",
        "            break\n",
        "        \n",
        "\n",
        "    return net_metrics"
      ],
      "metadata": {
        "id": "2aGrRuAlCmlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(M1, M2):\n",
        "    \"\"\"\n",
        "    Compute distance between matrices\n",
        "    Args:\n",
        "      M1 (numpy array): matrix1\n",
        "      M2 (numpy array): matrix2\n",
        "    Returns:\n",
        "      distance (float) between matrices\n",
        "    \"\"\"\n",
        "    return np.abs(np.array(M1) - np.array(M2)).sum()\n",
        "    \n",
        "def net_stability(net, ranks, eps):\n",
        "    \"\"\"\n",
        "    Compute network stability\n",
        "    Args:\n",
        "      net (DataFrame): temporal network\n",
        "      ranks (DataFrame): ranks of vertices by years\n",
        "      eps (float): epsilon\n",
        "    Returns:\n",
        "      stability (list): list of stability\n",
        "      distance_R (list): list of distances between ranks marices\n",
        "      distance_C (list): list of distances between matrices of influence\n",
        "      R (list): rank matrices\n",
        "      C (list): matrices of influence \n",
        "    \"\"\"\n",
        "\n",
        "    stability = []\n",
        "    distance_C = []\n",
        "    distance_R = []\n",
        "    C = []\n",
        "    R = []\n",
        "\n",
        "    for year in tqdm(range(2015, 2022)):\n",
        "        net_cur = net[net['Y'] == year]\n",
        "\n",
        "        # pkl_file = open(f'drive/MyDrive/Stability/R_{eps}_{year}.pkl', 'rb')\n",
        "        # R_t = pickle.load(pkl_file)\n",
        "        # pkl_file.close()\n",
        "\n",
        "        R_t = []\n",
        "        for i in tqdm(range(len(ranks))):\n",
        "            row_t = []\n",
        "            for j in range(len(ranks)):\n",
        "                rt = int(ranks.loc[i, str(year)] - ranks.loc[j, str(year)] > eps)\n",
        "                row_t.append(rt)\n",
        "\n",
        "            R_t.append(row_t)\n",
        "\n",
        "        R.append(R_t)\n",
        "\n",
        "        output = open(f'drive/MyDrive/Stability/R_{eps}_{year}.pkl', 'wb')\n",
        "        pickle.dump(R_t, output)\n",
        "        output.close()\n",
        "\n",
        "        # pkl_file = open(f'drive/MyDrive/Stability/C_{eps}_{year}.pkl', 'rb')\n",
        "        # C_t = pickle.load(pkl_file)\n",
        "        # pkl_file.close()\n",
        "\n",
        "        C_t = []\n",
        "\n",
        "        for i in tqdm(range(len(ranks))):\n",
        "            af_i = ranks.loc[i, 'AfId']\n",
        "            \n",
        "            if af_i not in net_cur['AfId1'].values:\n",
        "                C_t.append([0] * len(ranks))\n",
        "                continue\n",
        "\n",
        "            row_t = []\n",
        "            for j in range(len(ranks)):\n",
        "                af_j = ranks.loc[j, 'AfId']\n",
        "                if af_j not in net_cur['AfId2'].values:\n",
        "                    row_t.append(0)\n",
        "                else:\n",
        "                    net_tmp = net_cur[net_cur['AfId2'] == af_j]\n",
        "\n",
        "                    if len(net_tmp[net_tmp['AfId1'] == af_i]) == 0:\n",
        "                        row_t.append(0)\n",
        "                    else:\n",
        "                        row_t.append(net_tmp[net_tmp['AfId1'] == af_i]['weight'].values[0] / net_tmp['weight'].sum())\n",
        "                     \n",
        "            C_t.append(row_t)\n",
        "\n",
        "        C.append(C_t)\n",
        "\n",
        "        output = open(f'drive/MyDrive/Stability/C_{eps}_{year}.pkl', 'wb')\n",
        "        pickle.dump(C_t, output)\n",
        "        output.close()\n",
        "\n",
        "      \n",
        "    for i in tqdm(range(1, len(C))):\n",
        "        R_t = R[i-1]\n",
        "        R_t1 = R[i]\n",
        "        C_t = C[i-1]\n",
        "        C_t1 = C[i]\n",
        "        \n",
        "        gamma = max(np.max(C_t), np.max(C_t1))\n",
        "        d_C = distance(C_t, C_t1) / (len(ranks)**2 * gamma)\n",
        "        distance_C.append(d_C)\n",
        "\n",
        "        d_R = distance(R_t, R_t1) / (len(ranks) * (len(ranks) - 1))\n",
        "        distance_R.append(d_R)\n",
        "\n",
        "        res = np.sqrt((d_R**2 + d_C**2)/2)\n",
        "        stability.append(res)\n",
        "    \n",
        "    return stability, distance_R, distance_C, R, C\n"
      ],
      "metadata": {
        "id": "qMiCFP3mCTzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLwfcWO3vb3S"
      },
      "outputs": [],
      "source": [
        "# Community detection functions\n",
        "# Args:\n",
        "#   G (networkx graph)\n",
        "# Returns:\n",
        "#   comunities (list)\n",
        "\n",
        "# Louvain's community detection method\n",
        "def detect_communities_louvain(G):\n",
        "    partition = community_louvain.best_partition(G)\n",
        "    communities = list()\n",
        "    for com in set(partition.values()) :\n",
        "        list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
        "        communities.append(sorted(list_nodes))\n",
        "    return sorted(communities)\n",
        "  \n",
        "# Girvan Newman's community detection method\n",
        "def detect_communities_girvan_newman(G):\n",
        "    communities = community.girvan_newman(G)\n",
        "    return sorted(sorted(c) for c in communities)\n",
        "\n",
        "# Fast Greedy comunity detection method\n",
        "def detect_communities_greedy(G):\n",
        "    communities = community.greedy_modularity_communities(G)\n",
        "    return sorted(sorted(c) for c in communities)\n",
        "\n",
        "# Label, propogation community detection method\n",
        "def detect_communities_label_propagation(G):\n",
        "    communities = list()\n",
        "    for c in community.label_propagation_communities(G):\n",
        "        communities.append(sorted(c))\n",
        "    return sorted(communities)\n",
        "\n",
        "# Leiden community detection method\n",
        "def detect_communities_leiden(G):\n",
        "    communities = algorithms.leiden(G)\n",
        "    communities = json.loads(communities.to_json())['communities']\n",
        "    return communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgARrjKEWRVo"
      },
      "outputs": [],
      "source": [
        "def show_save_communities(G, communities, pos, name = 'Communities ', id_col = 'Id',\n",
        "                          folder = 'community', comm_file = 'communities.csv', image_file = 'image.png', text = ''):\n",
        "    \"\"\"\n",
        "    Draw graph with communties and save communities and image\n",
        "    Args:\n",
        "      G (networkx graph): graph\n",
        "      communities (list): list of comminities in graph\n",
        "      pos (list): positions of vertices\n",
        "      name (str): title\n",
        "      id_col (str): Id column\n",
        "      folder (str): name of folder to save  \n",
        "      comm_file (str): name of file to save communities \n",
        "      image_file (str): name of file to save image\n",
        "      text (str): text below image\n",
        "    \"\"\"\n",
        "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', '#F0F8FF', '#FAEBD7', '#00FFFF', '#7FFFD4', '#F0FFFF', '#F5F5DC', '#FFE4C4', '#000000', '#FFEBCD', '#0000FF', '#8A2BE2', '#A52A2A', '#DEB887', '#5F9EA0', '#7FFF00', '#D2691E', '#FF7F50', '#6495ED', '#FFF8DC', '#DC143C', '#00FFFF', '#00008B', '#008B8B', '#B8860B', '#A9A9A9', '#006400', '#A9A9A9', '#BDB76B', '#8B008B', '#556B2F', '#FF8C00', '#9932CC', '#8B0000', '#E9967A', '#8FBC8F', '#483D8B', '#2F4F4F', '#2F4F4F', '#00CED1', '#9400D3', '#FF1493', '#00BFFF', '#696969', '#696969', '#1E90FF', '#B22222', '#FFFAF0', '#228B22', '#FF00FF', '#DCDCDC', '#F8F8FF', '#FFD700', '#DAA520', '#808080', '#008000', '#ADFF2F', '#808080', '#F0FFF0', '#FF69B4', '#CD5C5C', '#4B0082', '#FFFFF0', '#F0E68C', '#E6E6FA', '#FFF0F5', '#7CFC00', '#FFFACD', '#ADD8E6', '#F08080', '#E0FFFF', '#FAFAD2', '#D3D3D3', '#90EE90', '#D3D3D3', '#FFB6C1', '#FFA07A', '#20B2AA', '#87CEFA', '#778899', '#778899', '#B0C4DE', '#FFFFE0', '#00FF00', '#32CD32', '#FAF0E6', '#FF00FF', '#800000', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#663399', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#F4A460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3', '#FFFFFF', '#F5F5F5', '#FFFF00', '#9ACD32'] * 1000\n",
        "    plt.figure(figsize = (14, 7))\n",
        "    plt.title(name, fontsize = 20)\n",
        "    aux = 0\n",
        "    community_df = pd.DataFrame()\n",
        "    print('show')\n",
        "    for community in communities:\n",
        "       nx.draw_networkx_nodes(G, pos, community, node_size = 20, node_color = colors[aux])\n",
        "       aux = aux + 1\n",
        "\n",
        "       for comm in community:\n",
        "           community_df.loc[comm, 'community'] = aux\n",
        "    \n",
        "    community_df = community_df.reset_index().rename(columns = {'index': id_col})\n",
        "    \n",
        "    if path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "    community_df.to_csv(folder + '/' + comm_file, sep = ';', index = False)\n",
        "    \n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
        "    plt.xlabel(text)\n",
        "    plt.savefig(folder + '/' + image_file)\n",
        "\n",
        "\n",
        "def find_save_communities(net, isolated = None, directed = False, id_col = 'Id', by_years = False, community_func = detect_communities_louvain,\n",
        "                          largest_comp = False, core = False, folder = 'community', method_name = '', communities = None):\n",
        "    \"\"\"\n",
        "    Find communities, show and save\n",
        "    Args:\n",
        "      net (DataFrame): network\n",
        "      isolated (DataFrame): isolated vertices\n",
        "      directed (bool): is graph directed\n",
        "      id_col (str): Id column\n",
        "      by_years (str): if True find communities by years \n",
        "      community_func (function): function to find communities\n",
        "      largest_comp (bool): if True find communities in largest connected component\n",
        "      core (int): if > 0 find comunities in core of graph\n",
        "      folder (str): foler to save\n",
        "      method_name (str): name of community detection method\n",
        "      communities (list): list of comunities\n",
        "    \"\"\"\n",
        "    net_no_periods = net.groupby([id_col + '1', id_col + '2']).sum()['weight'].reset_index()\n",
        "\n",
        "    if path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "\n",
        "    net_no_periods.to_csv(folder + '/' + 'net_comm.txt', sep = '\\t', index = False, header = False)\n",
        "\n",
        "    if directed: \n",
        "        G = nx.read_weighted_edgelist(folder + '/' + 'net_comm.txt', delimiter='\\t', create_using=nx.DiGraph, nodetype = str)\n",
        "    else:\n",
        "        G = nx.read_weighted_edgelist(folder + '/' + 'net_comm.txt', delimiter='\\t', nodetype = str)\n",
        "    if isolated is not None:\n",
        "        G.add_nodes_from(isolated[id_col])\n",
        "\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))    \n",
        "    print('start')\n",
        "    if largest_comp:\n",
        "        if directed:\n",
        "            components = nx.weakly_connected_components(G)\n",
        "        else:\n",
        "            components = nx.connected_components(G)\n",
        "        comp_df = pd.DataFrame()\n",
        "        i = 1\n",
        "        for comp in components:\n",
        "            for au in comp:\n",
        "                comp_df.loc[au, 'component'] = i\n",
        "            i += 1\n",
        "                    \n",
        "        comp_df = comp_df.reset_index().rename(columns = {'index': id_col})\n",
        "        cnt_comp = comp_df.groupby('component').count().reset_index().rename(columns = {id_col:'count'})\n",
        "        max_comp = cnt_comp[cnt_comp['count'] == np.max(cnt_comp['count'].values)]['component'].values[0]\n",
        "        print('comp')\n",
        "        S = G.subgraph(set(comp_df[comp_df['component'] == max_comp][id_col].values)).copy()\n",
        "    else:\n",
        "        S = G.copy()\n",
        "    if core:\n",
        "        S = nx.k_core(S, k = core)\n",
        "\n",
        "    pos = nx.spring_layout(S)\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', '#F0F8FF', '#FAEBD7', '#00FFFF', '#7FFFD4', '#F0FFFF', '#F5F5DC', '#FFE4C4', '#000000', '#FFEBCD', '#0000FF', '#8A2BE2', '#A52A2A', '#DEB887', '#5F9EA0', '#7FFF00', '#D2691E', '#FF7F50', '#6495ED', '#FFF8DC', '#DC143C', '#00FFFF', '#00008B', '#008B8B', '#B8860B', '#A9A9A9', '#006400', '#A9A9A9', '#BDB76B', '#8B008B', '#556B2F', '#FF8C00', '#9932CC', '#8B0000', '#E9967A', '#8FBC8F', '#483D8B', '#2F4F4F', '#2F4F4F', '#00CED1', '#9400D3', '#FF1493', '#00BFFF', '#696969', '#696969', '#1E90FF', '#B22222', '#FFFAF0', '#228B22', '#FF00FF', '#DCDCDC', '#F8F8FF', '#FFD700', '#DAA520', '#808080', '#008000', '#ADFF2F', '#808080', '#F0FFF0', '#FF69B4', '#CD5C5C', '#4B0082', '#FFFFF0', '#F0E68C', '#E6E6FA', '#FFF0F5', '#7CFC00', '#FFFACD', '#ADD8E6', '#F08080', '#E0FFFF', '#FAFAD2', '#D3D3D3', '#90EE90', '#D3D3D3', '#FFB6C1', '#FFA07A', '#20B2AA', '#87CEFA', '#778899', '#778899', '#B0C4DE', '#FFFFE0', '#00FF00', '#32CD32', '#FAF0E6', '#FF00FF', '#800000', '#66CDAA', '#0000CD', '#BA55D3', '#9370DB', '#3CB371', '#7B68EE', '#00FA9A', '#48D1CC', '#C71585', '#191970', '#F5FFFA', '#FFE4E1', '#FFE4B5', '#FFDEAD', '#000080', '#FDF5E6', '#808000', '#6B8E23', '#FFA500', '#FF4500', '#DA70D6', '#EEE8AA', '#98FB98', '#AFEEEE', '#DB7093', '#FFEFD5', '#FFDAB9', '#CD853F', '#FFC0CB', '#DDA0DD', '#B0E0E6', '#800080', '#663399', '#FF0000', '#BC8F8F', '#4169E1', '#8B4513', '#FA8072', '#F4A460', '#2E8B57', '#FFF5EE', '#A0522D', '#C0C0C0', '#87CEEB', '#6A5ACD', '#708090', '#708090', '#FFFAFA', '#00FF7F', '#4682B4', '#D2B48C', '#008080', '#D8BFD8', '#FF6347', '#40E0D0', '#EE82EE', '#F5DEB3', '#FFFFFF', '#F5F5F5', '#FFFF00', '#9ACD32'] * 1000\n",
        "    \n",
        "    if not by_years:\n",
        "        if communities is None:\n",
        "            communities = community_func(S)\n",
        "        show_save_communities(S, communities, pos, name = 'Communities ' + method_name, id_col = id_col,\n",
        "                          folder = folder, comm_file = 'communities.csv', image_file = 'image.png', text = f'{len(communities)} communities')\n",
        "        return\n",
        "\n",
        "    for year in range(2015, 2022):\n",
        "        net_year = net[net['Y'] == year]\n",
        "        net_year = net_year[(net_year[id_col + '1'].isin(list(map(int, S.nodes())))) | (net_year[id_col + '2'].isin(list(map(int, S.nodes()))))]\n",
        "        \n",
        "        net_year[[id_col + '1', id_col + '2', 'weight']].to_csv(folder + '/' + f'net_{year}.txt', sep = '\\t', index = False, header = False)\n",
        "\n",
        "        G_year = nx.read_weighted_edgelist(folder + '/' + f'net_{year}.txt', delimiter='\\t', nodetype = str)\n",
        "        if directed: \n",
        "            G_year = nx.read_weighted_edgelist(folder + '/' + f'net_{year}.txt', delimiter='\\t', create_using=nx.DiGraph, nodetype = str)\n",
        "        else:\n",
        "            G_year = nx.read_weighted_edgelist(folder + '/' + f'net_{year}.txt', delimiter='\\t', nodetype = str)\n",
        "\n",
        "        if isolated is not None:\n",
        "            isolated_year = isolated[isolated['Y'] == year]\n",
        "            G_year.add_nodes_from(isolated_year[id_col]) \n",
        "        G_year.remove_edges_from(nx.selfloop_edges(G_year))   \n",
        "\n",
        "        plt.figure(figsize = (15, 8))\n",
        "\n",
        "        aux = 0\n",
        "        communities = detect_communities_louvain(G_year)\n",
        "        \n",
        "        show_save_communities(G_year, communities, pos, name = 'Communities ' + method_name +  ' ' + str(year), id_col = id_col,\n",
        "                          folder = folder, comm_file = f'communities_{year}.csv', image_file = f'image_{year}.png', text = f'{len(communities)} communities')\n"
      ]
    }
  ]
}